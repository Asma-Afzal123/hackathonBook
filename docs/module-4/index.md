---
title: Module 4 - Vision-Language-Action Systems
sidebar_label: Introduction
slug: /module-4
---

# Module 4: Vision-Language-Action Systems for Autonomous Humanoids

Welcome to Module 4 of the AI-Native Book, where we explore Vision-Language-Action (VLA) systems for autonomous humanoid behavior. This module is designed for AI/robotics students familiar with ROS 2, simulation, and navigation concepts.

## Overview

This module covers how language models, perception, and robotics actions converge to create autonomous humanoid behavior:

- **[Voice-to-Action](./voice-to-action.md)** - Speech-to-text with OpenAI Whisper and translating voice commands into robot intents
<!-- - **[Cognitive Planning with LLMs](./llm-planning.md)** - Using LLMs for task decomposition and converting natural language goals into ROS 2 action sequences -->
<!-- - **[Capstone: The Autonomous Humanoid](./autonomous-humanoid.md)** - End-to-end VLA pipeline: Voice command → planning → navigation → perception → manipulation -->

## Learning Objectives

By the end of this module, you will be able to:
- Understand how speech-to-text systems convert voice commands into robot actions
- Explain how Large Language Models (LLMs) perform task decomposition
- Describe the process of converting natural language goals into ROS 2 action sequences
- Understand the complete Vision-Language-Action pipeline for autonomous humanoid behavior
- Trace the complete flow from voice command to final robot action

## Prerequisites

- Understanding of ROS 2 fundamentals (covered in Module 1)
- Simulation and navigation knowledge (covered in Module 2 and 3)

## Getting Started

Begin with the [Voice-to-Action](./voice-to-action.md) chapter to understand how voice commands are processed and translated into robot intents.